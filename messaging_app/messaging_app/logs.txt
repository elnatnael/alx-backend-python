
==> Audit <==
|---------|-------------------------|----------|---------|---------|---------------------|---------------------|
| Command |          Args           | Profile  |  User   | Version |     Start Time      |      End Time       |
|---------|-------------------------|----------|---------|---------|---------------------|---------------------|
| start   |                         | minikube | snowden | v1.36.0 | 14 Jul 25 13:57 EAT |                     |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 14 Jul 25 14:38 EAT |                     |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 14 Jul 25 14:41 EAT |                     |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 15 Jul 25 09:41 EAT | 15 Jul 25 10:14 EAT |
| start   | --driver=hyperkit       | minikube | snowden | v1.36.0 | 15 Jul 25 11:12 EAT |                     |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 15 Jul 25 11:12 EAT | 15 Jul 25 11:13 EAT |
| start   | --driver=hyperkit       | minikube | snowden | v1.36.0 | 15 Jul 25 13:01 EAT |                     |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 15 Jul 25 13:01 EAT |                     |
| start   | --driver=hyperkit       | minikube | snowden | v1.36.0 | 15 Jul 25 13:02 EAT |                     |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 15 Jul 25 13:02 EAT | 15 Jul 25 13:03 EAT |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 15 Jul 25 13:14 EAT | 15 Jul 25 13:15 EAT |
| service | messaging-service --url | minikube | snowden | v1.36.0 | 15 Jul 25 14:28 EAT |                     |
| start   | --driver=docker         | minikube | snowden | v1.36.0 | 15 Jul 25 15:07 EAT | 15 Jul 25 15:08 EAT |
| addons  | enable ingress          | minikube | snowden | v1.36.0 | 15 Jul 25 15:09 EAT |                     |
|---------|-------------------------|----------|---------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/07/15 15:07:58
Running on machine: snowdens-MacBook-Pro
Binary: Built with gc go1.24.0 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0715 15:07:58.514506   12215 out.go:345] Setting OutFile to fd 1 ...
I0715 15:07:58.516698   12215 out.go:397] isatty.IsTerminal(1) = true
I0715 15:07:58.516707   12215 out.go:358] Setting ErrFile to fd 2...
I0715 15:07:58.516717   12215 out.go:397] isatty.IsTerminal(2) = true
I0715 15:07:58.517380   12215 root.go:338] Updating PATH: /Users/snowden/.minikube/bin
W0715 15:07:58.518877   12215 root.go:314] Error reading config file at /Users/snowden/.minikube/config/config.json: open /Users/snowden/.minikube/config/config.json: no such file or directory
I0715 15:07:58.520866   12215 out.go:352] Setting JSON to false
I0715 15:07:58.571221   12215 start.go:130] hostinfo: {"hostname":"snowdens-MacBook-Pro.local","uptime":257402,"bootTime":1752323876,"procs":529,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"11.7.10","kernelVersion":"20.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"60cb1247-ee08-5ddf-a641-147784da4d89"}
W0715 15:07:58.576864   12215 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0715 15:07:58.587919   12215 out.go:177] 😄  minikube v1.36.0 on Darwin 11.7.10
I0715 15:07:58.601842   12215 notify.go:220] Checking for updates...
I0715 15:07:58.601999   12215 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0715 15:07:58.606103   12215 driver.go:404] Setting default libvirt URI to qemu:///system
I0715 15:07:59.555927   12215 docker.go:123] docker version: linux-20.10.13:Docker Desktop 4.6.1 (76265)
I0715 15:07:59.556882   12215 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0715 15:08:00.674631   12215 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.117928122s)
I0715 15:08:00.676174   12215 info.go:266] docker info: {ID:LWWG:DCBJ:ELTG:TDUN:XAXF:W7JD:G7JB:5MIK:RSWG:CFZP:MMZ6:QXND Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:false NGoroutines:47 SystemTime:2025-07-15 12:07:59.8137333 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125892608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.13 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc Expected:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.3.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0715 15:08:00.681759   12215 out.go:177] ✨  Using the docker driver based on existing profile
I0715 15:08:00.692104   12215 start.go:304] selected driver: docker
I0715 15:08:00.692119   12215 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0715 15:08:00.692193   12215 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0715 15:08:00.692473   12215 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0715 15:08:01.087107   12215 info.go:266] docker info: {ID:LWWG:DCBJ:ELTG:TDUN:XAXF:W7JD:G7JB:5MIK:RSWG:CFZP:MMZ6:QXND Containers:2 ContainersRunning:1 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:44 OomKillDisable:false NGoroutines:47 SystemTime:2025-07-15 12:08:00.9335122 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:4125892608 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.13 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc Expected:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.3.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0715 15:08:01.088275   12215 cni.go:84] Creating CNI manager for ""
I0715 15:08:01.088339   12215 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0715 15:08:01.088424   12215 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0715 15:08:01.101437   12215 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0715 15:08:01.124493   12215 cache.go:121] Beginning downloading kic base image for docker with docker
I0715 15:08:01.128198   12215 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0715 15:08:01.131772   12215 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0715 15:08:01.131898   12215 preload.go:146] Found local preload: /Users/snowden/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0715 15:08:01.131910   12215 cache.go:56] Caching tarball of preloaded images
I0715 15:08:01.132211   12215 image.go:81] Checking for docker.io/kicbase/stable:v0.0.47 in local docker daemon
I0715 15:08:01.136996   12215 preload.go:172] Found /Users/snowden/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0715 15:08:01.137740   12215 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0715 15:08:01.138076   12215 profile.go:143] Saving config to /Users/snowden/.minikube/profiles/minikube/config.json ...
I0715 15:08:01.454376   12215 image.go:100] Found docker.io/kicbase/stable:v0.0.47 in local docker daemon, skipping pull
I0715 15:08:01.454393   12215 cache.go:145] docker.io/kicbase/stable:v0.0.47 exists in daemon, skipping load
I0715 15:08:01.454413   12215 cache.go:230] Successfully downloaded all kic artifacts
I0715 15:08:01.455700   12215 start.go:360] acquireMachinesLock for minikube: {Name:mk453761f6eda114567ef79aeba43c3465f7ae08 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0715 15:08:01.455906   12215 start.go:364] duration metric: took 165.544µs to acquireMachinesLock for "minikube"
I0715 15:08:01.455946   12215 start.go:96] Skipping create...Using existing machine configuration
I0715 15:08:01.456641   12215 fix.go:54] fixHost starting: 
I0715 15:08:01.457161   12215 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0715 15:08:01.716982   12215 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0715 15:08:01.717039   12215 fix.go:138] unexpected machine state, will restart: <nil>
I0715 15:08:01.726507   12215 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0715 15:08:01.730287   12215 cli_runner.go:164] Run: docker start minikube
I0715 15:08:03.293865   12215 cli_runner.go:217] Completed: docker start minikube: (1.563714642s)
I0715 15:08:03.294191   12215 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0715 15:08:03.770057   12215 kic.go:430] container "minikube" state is running.
I0715 15:08:03.774762   12215 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0715 15:08:04.311536   12215 profile.go:143] Saving config to /Users/snowden/.minikube/profiles/minikube/config.json ...
I0715 15:08:04.312534   12215 machine.go:93] provisionDockerMachine start ...
I0715 15:08:04.314783   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:04.773476   12215 main.go:141] libmachine: Using SSH client type: native
I0715 15:08:04.781689   12215 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10dbce360] 0x10dbd1060 <nil>  [] 0s} 127.0.0.1 59725 <nil> <nil>}
I0715 15:08:04.781712   12215 main.go:141] libmachine: About to run SSH command:
hostname
I0715 15:08:05.321133   12215 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0715 15:08:05.322362   12215 ubuntu.go:169] provisioning hostname "minikube"
I0715 15:08:05.323284   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:05.704902   12215 main.go:141] libmachine: Using SSH client type: native
I0715 15:08:05.705432   12215 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10dbce360] 0x10dbd1060 <nil>  [] 0s} 127.0.0.1 59725 <nil> <nil>}
I0715 15:08:05.705448   12215 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0715 15:08:06.098083   12215 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0715 15:08:06.099016   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:06.458450   12215 main.go:141] libmachine: Using SSH client type: native
I0715 15:08:06.459193   12215 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10dbce360] 0x10dbd1060 <nil>  [] 0s} 127.0.0.1 59725 <nil> <nil>}
I0715 15:08:06.459218   12215 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0715 15:08:06.711139   12215 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0715 15:08:06.711266   12215 ubuntu.go:175] set auth options {CertDir:/Users/snowden/.minikube CaCertPath:/Users/snowden/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/snowden/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/snowden/.minikube/machines/server.pem ServerKeyPath:/Users/snowden/.minikube/machines/server-key.pem ClientKeyPath:/Users/snowden/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/snowden/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/snowden/.minikube}
I0715 15:08:06.711343   12215 ubuntu.go:177] setting up certificates
I0715 15:08:06.712717   12215 provision.go:84] configureAuth start
I0715 15:08:06.712973   12215 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0715 15:08:07.148112   12215 provision.go:143] copyHostCerts
I0715 15:08:07.157098   12215 exec_runner.go:144] found /Users/snowden/.minikube/key.pem, removing ...
I0715 15:08:07.159901   12215 exec_runner.go:203] rm: /Users/snowden/.minikube/key.pem
I0715 15:08:07.160515   12215 exec_runner.go:151] cp: /Users/snowden/.minikube/certs/key.pem --> /Users/snowden/.minikube/key.pem (1675 bytes)
I0715 15:08:07.162090   12215 exec_runner.go:144] found /Users/snowden/.minikube/ca.pem, removing ...
I0715 15:08:07.162103   12215 exec_runner.go:203] rm: /Users/snowden/.minikube/ca.pem
I0715 15:08:07.162248   12215 exec_runner.go:151] cp: /Users/snowden/.minikube/certs/ca.pem --> /Users/snowden/.minikube/ca.pem (1082 bytes)
I0715 15:08:07.162927   12215 exec_runner.go:144] found /Users/snowden/.minikube/cert.pem, removing ...
I0715 15:08:07.162935   12215 exec_runner.go:203] rm: /Users/snowden/.minikube/cert.pem
I0715 15:08:07.163083   12215 exec_runner.go:151] cp: /Users/snowden/.minikube/certs/cert.pem --> /Users/snowden/.minikube/cert.pem (1123 bytes)
I0715 15:08:07.163705   12215 provision.go:117] generating server cert: /Users/snowden/.minikube/machines/server.pem ca-key=/Users/snowden/.minikube/certs/ca.pem private-key=/Users/snowden/.minikube/certs/ca-key.pem org=snowden.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0715 15:08:07.281250   12215 provision.go:177] copyRemoteCerts
I0715 15:08:07.282911   12215 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0715 15:08:07.283189   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:07.656004   12215 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59725 SSHKeyPath:/Users/snowden/.minikube/machines/minikube/id_rsa Username:docker}
I0715 15:08:07.826578   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0715 15:08:07.901228   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0715 15:08:07.964725   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0715 15:08:08.035733   12215 provision.go:87] duration metric: took 1.322152605s to configureAuth
I0715 15:08:08.035764   12215 ubuntu.go:193] setting minikube options for container-runtime
I0715 15:08:08.036122   12215 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0715 15:08:08.036354   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:08.436215   12215 main.go:141] libmachine: Using SSH client type: native
I0715 15:08:08.436796   12215 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10dbce360] 0x10dbd1060 <nil>  [] 0s} 127.0.0.1 59725 <nil> <nil>}
I0715 15:08:08.436805   12215 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0715 15:08:08.808584   12215 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0715 15:08:08.808602   12215 ubuntu.go:71] root file system type: overlay
I0715 15:08:08.810451   12215 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0715 15:08:08.810683   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:09.452697   12215 main.go:141] libmachine: Using SSH client type: native
I0715 15:08:09.453088   12215 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10dbce360] 0x10dbd1060 <nil>  [] 0s} 127.0.0.1 59725 <nil> <nil>}
I0715 15:08:09.453181   12215 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0715 15:08:09.704189   12215 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0715 15:08:09.704948   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:10.059193   12215 main.go:141] libmachine: Using SSH client type: native
I0715 15:08:10.059567   12215 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10dbce360] 0x10dbd1060 <nil>  [] 0s} 127.0.0.1 59725 <nil> <nil>}
I0715 15:08:10.059600   12215 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0715 15:08:10.273209   12215 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0715 15:08:10.273513   12215 machine.go:96] duration metric: took 5.961665812s to provisionDockerMachine
I0715 15:08:10.274161   12215 start.go:293] postStartSetup for "minikube" (driver="docker")
I0715 15:08:10.274193   12215 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0715 15:08:10.274429   12215 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0715 15:08:10.274637   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:10.581277   12215 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59725 SSHKeyPath:/Users/snowden/.minikube/machines/minikube/id_rsa Username:docker}
I0715 15:08:10.714306   12215 ssh_runner.go:195] Run: cat /etc/os-release
I0715 15:08:10.736059   12215 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0715 15:08:10.736102   12215 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0715 15:08:10.736117   12215 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0715 15:08:10.736129   12215 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0715 15:08:10.736602   12215 filesync.go:126] Scanning /Users/snowden/.minikube/addons for local assets ...
I0715 15:08:10.737571   12215 filesync.go:126] Scanning /Users/snowden/.minikube/files for local assets ...
I0715 15:08:10.737694   12215 start.go:296] duration metric: took 463.560593ms for postStartSetup
I0715 15:08:10.738544   12215 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0715 15:08:10.738696   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:11.029844   12215 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59725 SSHKeyPath:/Users/snowden/.minikube/machines/minikube/id_rsa Username:docker}
I0715 15:08:11.147822   12215 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0715 15:08:11.165303   12215 fix.go:56] duration metric: took 9.71057459s for fixHost
I0715 15:08:11.165329   12215 start.go:83] releasing machines lock for "minikube", held for 9.71065017s
I0715 15:08:11.166344   12215 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0715 15:08:11.451187   12215 ssh_runner.go:195] Run: cat /version.json
I0715 15:08:11.451397   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:11.456342   12215 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0715 15:08:11.458306   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:11.823568   12215 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59725 SSHKeyPath:/Users/snowden/.minikube/machines/minikube/id_rsa Username:docker}
I0715 15:08:11.835426   12215 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59725 SSHKeyPath:/Users/snowden/.minikube/machines/minikube/id_rsa Username:docker}
I0715 15:08:12.721449   12215 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.265163001s)
I0715 15:08:12.721588   12215 ssh_runner.go:235] Completed: cat /version.json: (1.270456293s)
I0715 15:08:12.725985   12215 ssh_runner.go:195] Run: systemctl --version
I0715 15:08:12.751133   12215 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0715 15:08:12.773282   12215 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0715 15:08:12.827707   12215 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0715 15:08:12.827971   12215 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0715 15:08:12.849923   12215 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0715 15:08:12.849941   12215 start.go:495] detecting cgroup driver to use...
I0715 15:08:12.851005   12215 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0715 15:08:12.855016   12215 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0715 15:08:12.886256   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0715 15:08:12.908260   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0715 15:08:12.935919   12215 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0715 15:08:12.936221   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0715 15:08:12.958802   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0715 15:08:12.982688   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0715 15:08:13.010124   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0715 15:08:13.031672   12215 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0715 15:08:13.052953   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0715 15:08:13.082660   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0715 15:08:13.106139   12215 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0715 15:08:13.131241   12215 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0715 15:08:13.153715   12215 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0715 15:08:13.177424   12215 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0715 15:08:13.343020   12215 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0715 15:08:13.600592   12215 start.go:495] detecting cgroup driver to use...
I0715 15:08:13.600620   12215 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0715 15:08:13.601772   12215 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0715 15:08:13.685198   12215 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0715 15:08:13.685380   12215 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0715 15:08:13.720325   12215 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0715 15:08:13.768347   12215 ssh_runner.go:195] Run: which cri-dockerd
I0715 15:08:13.787847   12215 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0715 15:08:13.814799   12215 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0715 15:08:13.873582   12215 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0715 15:08:14.162965   12215 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0715 15:08:14.420436   12215 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0715 15:08:14.423439   12215 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0715 15:08:14.482508   12215 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0715 15:08:14.518894   12215 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0715 15:08:14.718027   12215 ssh_runner.go:195] Run: sudo systemctl restart docker
I0715 15:08:15.949214   12215 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.231219876s)
I0715 15:08:15.949514   12215 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0715 15:08:15.975080   12215 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0715 15:08:16.006225   12215 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0715 15:08:16.032613   12215 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0715 15:08:16.177636   12215 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0715 15:08:16.332135   12215 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0715 15:08:16.470965   12215 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0715 15:08:16.505785   12215 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0715 15:08:16.531038   12215 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0715 15:08:16.669706   12215 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0715 15:08:17.230859   12215 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0715 15:08:17.265247   12215 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0715 15:08:17.265904   12215 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0715 15:08:17.280124   12215 start.go:563] Will wait 60s for crictl version
I0715 15:08:17.280331   12215 ssh_runner.go:195] Run: which crictl
I0715 15:08:17.294059   12215 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0715 15:08:17.607492   12215 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0715 15:08:17.607687   12215 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0715 15:08:17.898132   12215 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0715 15:08:17.970308   12215 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0715 15:08:17.972299   12215 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0715 15:08:18.627494   12215 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0715 15:08:18.629425   12215 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0715 15:08:18.639595   12215 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0715 15:08:18.669304   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0715 15:08:18.938894   12215 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0715 15:08:18.939831   12215 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0715 15:08:18.940037   12215 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0715 15:08:19.007855   12215 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0715 15:08:19.007884   12215 docker.go:632] Images already preloaded, skipping extraction
I0715 15:08:19.008889   12215 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0715 15:08:19.067868   12215 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0715 15:08:19.068601   12215 cache_images.go:84] Images are preloaded, skipping loading
I0715 15:08:19.068979   12215 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0715 15:08:19.070634   12215 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0715 15:08:19.070811   12215 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0715 15:08:19.576802   12215 cni.go:84] Creating CNI manager for ""
I0715 15:08:19.576826   12215 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0715 15:08:19.577390   12215 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0715 15:08:19.577438   12215 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0715 15:08:19.577718   12215 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0715 15:08:19.577978   12215 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0715 15:08:19.605937   12215 binaries.go:44] Found k8s binaries, skipping transfer
I0715 15:08:19.606146   12215 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0715 15:08:19.632350   12215 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0715 15:08:19.680144   12215 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0715 15:08:19.721479   12215 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0715 15:08:19.775289   12215 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0715 15:08:19.789700   12215 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0715 15:08:19.820098   12215 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0715 15:08:19.983640   12215 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0715 15:08:20.032546   12215 certs.go:68] Setting up /Users/snowden/.minikube/profiles/minikube for IP: 192.168.49.2
I0715 15:08:20.032562   12215 certs.go:194] generating shared ca certs ...
I0715 15:08:20.032582   12215 certs.go:226] acquiring lock for ca certs: {Name:mk1c67e4da68d23b0a161cd8a6e5c36b2c947386 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0715 15:08:20.034709   12215 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/snowden/.minikube/ca.key
I0715 15:08:20.035127   12215 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/snowden/.minikube/proxy-client-ca.key
I0715 15:08:20.035472   12215 certs.go:256] generating profile certs ...
I0715 15:08:20.036208   12215 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/snowden/.minikube/profiles/minikube/client.key
I0715 15:08:20.036754   12215 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/snowden/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0715 15:08:20.037468   12215 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/snowden/.minikube/profiles/minikube/proxy-client.key
I0715 15:08:20.038187   12215 certs.go:484] found cert: /Users/snowden/.minikube/certs/ca-key.pem (1675 bytes)
I0715 15:08:20.038502   12215 certs.go:484] found cert: /Users/snowden/.minikube/certs/ca.pem (1082 bytes)
I0715 15:08:20.038761   12215 certs.go:484] found cert: /Users/snowden/.minikube/certs/cert.pem (1123 bytes)
I0715 15:08:20.039182   12215 certs.go:484] found cert: /Users/snowden/.minikube/certs/key.pem (1675 bytes)
I0715 15:08:20.051671   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0715 15:08:20.237636   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0715 15:08:20.319979   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0715 15:08:20.410969   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0715 15:08:20.504097   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0715 15:08:20.645991   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0715 15:08:20.757346   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0715 15:08:20.850841   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0715 15:08:21.016829   12215 ssh_runner.go:362] scp /Users/snowden/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0715 15:08:21.085337   12215 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0715 15:08:21.151610   12215 ssh_runner.go:195] Run: openssl version
I0715 15:08:21.181299   12215 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0715 15:08:21.217515   12215 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0715 15:08:21.236231   12215 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 15 07:13 /usr/share/ca-certificates/minikubeCA.pem
I0715 15:08:21.236420   12215 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0715 15:08:21.294104   12215 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0715 15:08:21.337969   12215 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0715 15:08:21.360657   12215 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0715 15:08:21.402350   12215 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0715 15:08:21.449627   12215 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0715 15:08:21.497184   12215 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0715 15:08:21.535878   12215 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0715 15:08:21.560454   12215 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0715 15:08:21.606680   12215 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.47 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0715 15:08:21.607237   12215 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0715 15:08:21.763369   12215 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0715 15:08:21.803864   12215 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0715 15:08:21.805510   12215 kubeadm.go:589] restartPrimaryControlPlane start ...
I0715 15:08:21.805708   12215 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0715 15:08:21.870005   12215 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0715 15:08:21.870350   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0715 15:08:22.295208   12215 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:57938"
I0715 15:08:22.295839   12215 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:57938, want: 127.0.0.1:59724
I0715 15:08:22.296935   12215 kubeconfig.go:62] /Users/snowden/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0715 15:08:22.298540   12215 lock.go:35] WriteFile acquiring /Users/snowden/.kube/config: {Name:mkd2a2595c1171ed9aff6d41f26142e76225cc84 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0715 15:08:22.352692   12215 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0715 15:08:22.438582   12215 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0715 15:08:22.438629   12215 kubeadm.go:593] duration metric: took 633.13077ms to restartPrimaryControlPlane
I0715 15:08:22.438641   12215 kubeadm.go:394] duration metric: took 832.012226ms to StartCluster
I0715 15:08:22.439480   12215 settings.go:142] acquiring lock: {Name:mk1792b3677e581a21eb3917c768374cf8f16489 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0715 15:08:22.439727   12215 settings.go:150] Updating kubeconfig:  /Users/snowden/.kube/config
I0715 15:08:22.444134   12215 lock.go:35] WriteFile acquiring /Users/snowden/.kube/config: {Name:mkd2a2595c1171ed9aff6d41f26142e76225cc84 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0715 15:08:22.446383   12215 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0715 15:08:22.446777   12215 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0715 15:08:22.448651   12215 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0715 15:08:22.450536   12215 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0715 15:08:22.450530   12215 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0715 15:08:22.450575   12215 out.go:177] 🔎  Verifying Kubernetes components...
I0715 15:08:22.451078   12215 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0715 15:08:22.451264   12215 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0715 15:08:22.451826   12215 addons.go:247] addon storage-provisioner should already be in state true
I0715 15:08:22.452385   12215 host.go:66] Checking if "minikube" exists ...
I0715 15:08:22.457917   12215 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0715 15:08:22.458107   12215 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0715 15:08:22.459836   12215 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0715 15:08:22.872876   12215 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0715 15:08:22.876130   12215 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0715 15:08:22.876147   12215 addons.go:247] addon default-storageclass should already be in state true
I0715 15:08:22.876186   12215 host.go:66] Checking if "minikube" exists ...
I0715 15:08:22.877757   12215 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0715 15:08:22.877769   12215 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0715 15:08:22.878020   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:22.878170   12215 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0715 15:08:23.247315   12215 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59725 SSHKeyPath:/Users/snowden/.minikube/machines/minikube/id_rsa Username:docker}
I0715 15:08:23.278149   12215 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0715 15:08:23.278168   12215 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0715 15:08:23.278452   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0715 15:08:23.497486   12215 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.037613283s)
I0715 15:08:23.497727   12215 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0715 15:08:23.588858   12215 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0715 15:08:23.657379   12215 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59725 SSHKeyPath:/Users/snowden/.minikube/machines/minikube/id_rsa Username:docker}
I0715 15:08:24.026749   12215 api_server.go:52] waiting for apiserver process to appear ...
I0715 15:08:24.027064   12215 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0715 15:08:24.529280   12215 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0715 15:08:24.535430   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0715 15:08:24.817615   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0715 15:08:25.032555   12215 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0715 15:08:26.494932   12215 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.959518998s)
W0715 15:08:26.494972   12215 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:26.495971   12215 retry.go:31] will retry after 167.779306ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:26.504698   12215 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.472152879s)
I0715 15:08:26.505016   12215 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0715 15:08:26.535912   12215 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.718313149s)
W0715 15:08:26.535949   12215 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:26.535967   12215 retry.go:31] will retry after 193.494719ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:26.551565   12215 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0715 15:08:26.664416   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0715 15:08:26.730778   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0715 15:08:27.248406   12215 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0715 15:08:28.469730   12215 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.805328693s)
W0715 15:08:28.469754   12215 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:28.469769   12215 retry.go:31] will retry after 487.129688ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:28.469842   12215 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.739088537s)
W0715 15:08:28.469856   12215 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:28.469863   12215 retry.go:31] will retry after 365.80925ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:28.469889   12215 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.221496782s)
I0715 15:08:28.470109   12215 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0715 15:08:28.530311   12215 api_server.go:72] duration metric: took 6.084034981s to wait for apiserver process to appear ...
I0715 15:08:28.530330   12215 api_server.go:88] waiting for apiserver healthz status ...
I0715 15:08:28.538716   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:28.619396   12215 api_server.go:269] stopped: https://127.0.0.1:59724/healthz: Get "https://127.0.0.1:59724/healthz": EOF
I0715 15:08:28.836617   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0715 15:08:28.962109   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0715 15:08:29.030774   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:29.039728   12215 api_server.go:269] stopped: https://127.0.0.1:59724/healthz: Get "https://127.0.0.1:59724/healthz": EOF
I0715 15:08:29.532517   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:29.538967   12215 api_server.go:269] stopped: https://127.0.0.1:59724/healthz: Get "https://127.0.0.1:59724/healthz": EOF
W0715 15:08:29.674439   12215 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:29.674458   12215 retry.go:31] will retry after 312.696777ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0715 15:08:29.883563   12215 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:29.883598   12215 retry.go:31] will retry after 529.262927ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:29.989891   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0715 15:08:30.031229   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:30.035579   12215 api_server.go:269] stopped: https://127.0.0.1:59724/healthz: Get "https://127.0.0.1:59724/healthz": EOF
W0715 15:08:30.322252   12215 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:30.322276   12215 retry.go:31] will retry after 482.991986ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0715 15:08:30.414021   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0715 15:08:30.530871   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:30.808747   12215 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0715 15:08:35.535293   12215 api_server.go:269] stopped: https://127.0.0.1:59724/healthz: Get "https://127.0.0.1:59724/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0715 15:08:35.535326   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:37.633778   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0715 15:08:37.633803   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0715 15:08:37.633819   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:37.777040   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0715 15:08:37.777063   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0715 15:08:38.032361   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:38.152330   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0715 15:08:38.152351   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0715 15:08:38.531542   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:38.651621   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:38.651650   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:39.030466   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:39.065207   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:39.065238   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:39.531202   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:39.556715   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:39.556751   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:39.780783   12215 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (9.366829971s)
I0715 15:08:39.781416   12215 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (8.972725484s)
I0715 15:08:39.926798   12215 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0715 15:08:39.937492   12215 addons.go:514] duration metric: took 17.491410096s for enable addons: enabled=[storage-provisioner default-storageclass]
I0715 15:08:40.031090   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:40.087194   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:40.087277   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:40.531162   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:40.594609   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:40.594631   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:41.032813   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:41.084829   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:41.084857   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:41.530830   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:41.572281   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:41.572310   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:42.030326   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:42.121326   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:42.121367   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:42.532282   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:42.683988   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:42.684013   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:43.030665   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:43.108349   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:43.108369   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:43.530885   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:43.639542   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:43.639568   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:44.034783   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:44.253639   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:44.253674   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:44.530775   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:44.587151   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:44.587363   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:45.036536   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:45.446379   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:45.446411   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:45.539965   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:45.785039   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:45.785069   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:46.068489   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:46.457080   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:46.457120   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:46.530831   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:46.854172   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:46.854204   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:47.030488   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:47.165782   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:47.165821   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:47.530310   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:47.614188   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0715 15:08:47.614212   12215 api_server.go:103] status: https://127.0.0.1:59724/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0715 15:08:48.032502   12215 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59724/healthz ...
I0715 15:08:48.061401   12215 api_server.go:279] https://127.0.0.1:59724/healthz returned 200:
ok
I0715 15:08:48.067788   12215 api_server.go:141] control plane version: v1.33.1
I0715 15:08:48.067838   12215 api_server.go:131] duration metric: took 19.537635905s to wait for apiserver health ...
I0715 15:08:48.069295   12215 system_pods.go:43] waiting for kube-system pods to appear ...
I0715 15:08:48.188777   12215 system_pods.go:59] 8 kube-system pods found
I0715 15:08:48.188858   12215 system_pods.go:61] "coredns-674b8bbfcf-4h6gp" [fec912aa-9f66-47c2-94d0-c905dbcbdcf4] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0715 15:08:48.188868   12215 system_pods.go:61] "coredns-674b8bbfcf-ldbpq" [1f476f06-f74d-4672-b72a-e6e6d2e1ffc5] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0715 15:08:48.188874   12215 system_pods.go:61] "etcd-minikube" [90ab415f-7660-443a-9d29-eb6dd73fa83e] Running
I0715 15:08:48.188881   12215 system_pods.go:61] "kube-apiserver-minikube" [a3a6c917-7fa8-499c-aef1-37e312985103] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0715 15:08:48.188890   12215 system_pods.go:61] "kube-controller-manager-minikube" [0a70ec5c-3a7d-4163-981b-2487d78ccfae] Running
I0715 15:08:48.188896   12215 system_pods.go:61] "kube-proxy-vcddx" [b295f633-7a71-429f-a6dd-eae4191baf23] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0715 15:08:48.188901   12215 system_pods.go:61] "kube-scheduler-minikube" [d438cc23-1d3e-4045-b52e-a3944a1adb32] Running
I0715 15:08:48.188911   12215 system_pods.go:61] "storage-provisioner" [cb5f24e1-03af-4253-9d98-28787afe6bbd] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0715 15:08:48.188916   12215 system_pods.go:74] duration metric: took 119.611603ms to wait for pod list to return data ...
I0715 15:08:48.188931   12215 kubeadm.go:578] duration metric: took 25.742803949s to wait for: map[apiserver:true system_pods:true]
I0715 15:08:48.188970   12215 node_conditions.go:102] verifying NodePressure condition ...
I0715 15:08:48.217995   12215 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0715 15:08:48.218465   12215 node_conditions.go:123] node cpu capacity is 2
I0715 15:08:48.219527   12215 node_conditions.go:105] duration metric: took 30.540033ms to run NodePressure ...
I0715 15:08:48.219551   12215 start.go:241] waiting for startup goroutines ...
I0715 15:08:48.219576   12215 start.go:246] waiting for cluster config update ...
I0715 15:08:48.219629   12215 start.go:255] writing updated cluster config ...
I0715 15:08:48.223050   12215 ssh_runner.go:195] Run: rm -f paused
I0715 15:08:49.074844   12215 start.go:607] kubectl: 1.22.5, cluster: 1.33.1 (minor skew: 11)
I0715 15:08:49.527546   12215 out.go:201] 
W0715 15:08:49.539934   12215 out.go:270] ❗  /usr/local/bin/kubectl is version 1.22.5, which may have incompatibilities with Kubernetes 1.33.1.
I0715 15:08:49.549411   12215 out.go:177]     ▪ Want kubectl v1.33.1? Try 'minikube kubectl -- get pods -A'
I0715 15:08:49.554516   12215 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jul 15 12:08:38 minikube cri-dockerd[1304]: time="2025-07-15T12:08:38Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jul 15 12:08:41 minikube cri-dockerd[1304]: time="2025-07-15T12:08:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7c0fdda14eb01049674c82b4f0ab7b3b622cae9e27576c589e24e150d14ef71a/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Jul 15 12:08:42 minikube cri-dockerd[1304]: time="2025-07-15T12:08:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d815e0ec4e7ced08c5d1969a6402e852ee0dd1a31f5b64564ca406bd575c4d78/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Jul 15 12:08:45 minikube cri-dockerd[1304]: time="2025-07-15T12:08:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/80b3aefd6bd08aee6e3f770b717708a9613219309359f8c507af286f32553b7c/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Jul 15 12:08:47 minikube cri-dockerd[1304]: time="2025-07-15T12:08:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/87b1423b519c210ea4d38859c40806c0ee8a4f492e20e507c80676f1cb0fb8d8/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Jul 15 12:08:47 minikube cri-dockerd[1304]: time="2025-07-15T12:08:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/37dfcd980d0248270217b76d0b2fb4534c92dff4acf18b7d56cb37b29a2c85bb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 15 12:08:47 minikube dockerd[997]: time="2025-07-15T12:08:47.980273900Z" level=info msg="ignoring event" container=0f7d7478589b3016cfde83e41febe0f1c1b52a19c1e5662af495d16e81f73f2a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 15 12:08:48 minikube cri-dockerd[1304]: time="2025-07-15T12:08:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8cbcad9900338e9f83049b7c3378ab31e24394f9dd1294bdd49b59b3acc495c6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 15 12:08:48 minikube cri-dockerd[1304]: time="2025-07-15T12:08:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47814bdf0e73ef8a94d2fd7de13fe3227050330555e317c11fc8574bedeede4b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 15 12:09:20 minikube dockerd[997]: time="2025-07-15T12:09:20.299412200Z" level=info msg="ignoring event" container=c5854b86f6c5082d212cf4a7f10f797fcfd2a5759ad8bf5fe4ad1cb817433017 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 15 12:09:30 minikube cri-dockerd[1304]: time="2025-07-15T12:09:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/70c0bba98dc71ec5e4288a67632806814245ee95f8710c60f3b919156013ab74/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 15 12:09:30 minikube cri-dockerd[1304]: time="2025-07-15T12:09:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/658e5343b0455de80962ba5d7005882e4c0c1c39a3172c375797be265b1b0e39/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 15 12:09:31 minikube dockerd[997]: time="2025-07-15T12:09:31.784869200Z" level=warning msg="reference for unknown type: " digest="sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Jul 15 12:09:43 minikube cri-dockerd[1304]: time="2025-07-15T12:09:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: Pulling from ingress-nginx/kube-webhook-certgen"
Jul 15 12:09:53 minikube cri-dockerd[1304]: time="2025-07-15T12:09:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: Pulling from ingress-nginx/kube-webhook-certgen"
Jul 15 12:10:03 minikube cri-dockerd[1304]: time="2025-07-15T12:10:03Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [>                                                  ]  278.7kB/26.67MB"
Jul 15 12:10:13 minikube cri-dockerd[1304]: time="2025-07-15T12:10:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [=>                                                 ]  557.3kB/26.67MB"
Jul 15 12:10:23 minikube cri-dockerd[1304]: time="2025-07-15T12:10:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [=>                                                 ]  835.8kB/26.67MB"
Jul 15 12:10:33 minikube cri-dockerd[1304]: time="2025-07-15T12:10:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [==>                                                ]  1.114MB/26.67MB"
Jul 15 12:10:43 minikube cri-dockerd[1304]: time="2025-07-15T12:10:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [==>                                                ]  1.393MB/26.67MB"
Jul 15 12:10:53 minikube cri-dockerd[1304]: time="2025-07-15T12:10:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [===>                                               ]  1.671MB/26.67MB"
Jul 15 12:11:03 minikube cri-dockerd[1304]: time="2025-07-15T12:11:03Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [===>                                               ]   1.95MB/26.67MB"
Jul 15 12:11:13 minikube cri-dockerd[1304]: time="2025-07-15T12:11:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====>                                              ]  2.228MB/26.67MB"
Jul 15 12:11:23 minikube cri-dockerd[1304]: time="2025-07-15T12:11:23Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====>                                              ]  2.507MB/26.67MB"
Jul 15 12:11:33 minikube cri-dockerd[1304]: time="2025-07-15T12:11:33Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====>                                              ]  2.507MB/26.67MB"
Jul 15 12:11:43 minikube cri-dockerd[1304]: time="2025-07-15T12:11:43Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====>                                              ]  2.507MB/26.67MB"
Jul 15 12:11:53 minikube cri-dockerd[1304]: time="2025-07-15T12:11:53Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====>                                              ]  2.507MB/26.67MB"
Jul 15 12:12:03 minikube cri-dockerd[1304]: time="2025-07-15T12:12:03Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====>                                              ]  2.507MB/26.67MB"
Jul 15 12:12:13 minikube cri-dockerd[1304]: time="2025-07-15T12:12:13Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [====>                                              ]  2.507MB/26.67MB"
Jul 15 12:12:23 minikube cri-dockerd[1304]: time="2025-07-15T12:12:23Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524 because it exceeded image pull deadline 1m0s. Latest progress 8a610164bdae: Downloading [====>                                              ]  2.507MB/26.67MB"
Jul 15 12:12:23 minikube dockerd[997]: time="2025-07-15T12:12:23.350102800Z" level=info msg="Not continuing with pull after error" error="context canceled"
Jul 15 12:12:24 minikube dockerd[997]: time="2025-07-15T12:12:24.671351100Z" level=info msg="ignoring event" container=70c0bba98dc71ec5e4288a67632806814245ee95f8710c60f3b919156013ab74 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 15 12:12:25 minikube cri-dockerd[1304]: time="2025-07-15T12:12:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b50edb0bdc82c68e0ae4640379daaa67857ff798cab60f7684d42d5068c67ca7/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 15 12:12:34 minikube cri-dockerd[1304]: time="2025-07-15T12:12:34Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:12:44 minikube cri-dockerd[1304]: time="2025-07-15T12:12:44Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:12:54 minikube cri-dockerd[1304]: time="2025-07-15T12:12:54Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:13:04 minikube cri-dockerd[1304]: time="2025-07-15T12:13:04Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:13:14 minikube cri-dockerd[1304]: time="2025-07-15T12:13:14Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:13:24 minikube cri-dockerd[1304]: time="2025-07-15T12:13:24Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:13:34 minikube cri-dockerd[1304]: time="2025-07-15T12:13:34Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524 because it exceeded image pull deadline 1m0s. Latest progress 8a610164bdae: Pulling fs layer "
Jul 15 12:13:34 minikube dockerd[997]: time="2025-07-15T12:13:34.681199000Z" level=info msg="Not continuing with pull after error" error="context canceled"
Jul 15 12:13:35 minikube dockerd[997]: time="2025-07-15T12:13:35.102001400Z" level=info msg="ignoring event" container=658e5343b0455de80962ba5d7005882e4c0c1c39a3172c375797be265b1b0e39 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jul 15 12:13:35 minikube cri-dockerd[1304]: time="2025-07-15T12:13:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6bec8fe291e67b1676df56bef718bf5d9acb50e743a68fdc997ee135804f5def/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jul 15 12:13:47 minikube cri-dockerd[1304]: time="2025-07-15T12:13:47Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:13:57 minikube cri-dockerd[1304]: time="2025-07-15T12:13:57Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:14:07 minikube cri-dockerd[1304]: time="2025-07-15T12:14:07Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:14:17 minikube cri-dockerd[1304]: time="2025-07-15T12:14:17Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:14:26 minikube cri-dockerd[1304]: time="2025-07-15T12:14:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:14:36 minikube cri-dockerd[1304]: time="2025-07-15T12:14:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:14:46 minikube cri-dockerd[1304]: time="2025-07-15T12:14:46Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [>                                                  ]    277kB/26.67MB"
Jul 15 12:14:56 minikube cri-dockerd[1304]: time="2025-07-15T12:14:56Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [>                                                  ]    277kB/26.67MB"
Jul 15 12:15:06 minikube cri-dockerd[1304]: time="2025-07-15T12:15:06Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [>                                                  ]    277kB/26.67MB"
Jul 15 12:15:16 minikube cri-dockerd[1304]: time="2025-07-15T12:15:16Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [>                                                  ]    277kB/26.67MB"
Jul 15 12:15:26 minikube cri-dockerd[1304]: time="2025-07-15T12:15:26Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [>                                                  ]    277kB/26.67MB"
Jul 15 12:15:36 minikube cri-dockerd[1304]: time="2025-07-15T12:15:36Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Downloading [>                                                  ]    277kB/26.67MB"
Jul 15 12:15:46 minikube cri-dockerd[1304]: time="2025-07-15T12:15:46Z" level=error msg="Cancel pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524 because it exceeded image pull deadline 1m0s. Latest progress 8a610164bdae: Downloading [>                                                  ]    277kB/26.67MB"
Jul 15 12:15:46 minikube dockerd[997]: time="2025-07-15T12:15:46.928523600Z" level=info msg="Not continuing with pull after error" error="context canceled"
Jul 15 12:15:58 minikube cri-dockerd[1304]: time="2025-07-15T12:15:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:16:08 minikube cri-dockerd[1304]: time="2025-07-15T12:16:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "
Jul 15 12:16:18 minikube cri-dockerd[1304]: time="2025-07-15T12:16:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524: 8a610164bdae: Pulling fs layer "


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
7539bb12576d3       6e38f40d628db       6 minutes ago       Running             storage-provisioner       7                   80b3aefd6bd08       storage-provisioner
2407846648db5       ef43894fa110c       7 minutes ago       Running             kube-controller-manager   6                   3df1995ebac06       kube-controller-manager-minikube
3e8394f6cc357       b79c189b052cd       7 minutes ago       Running             kube-proxy                4                   87b1423b519c2       kube-proxy-vcddx
c5854b86f6c50       6e38f40d628db       7 minutes ago       Exited              storage-provisioner       6                   80b3aefd6bd08       storage-provisioner
33e494c341a50       1cf5f116067c6       7 minutes ago       Running             coredns                   4                   d815e0ec4e7ce       coredns-674b8bbfcf-ldbpq
a17957bb290a2       1cf5f116067c6       7 minutes ago       Running             coredns                   4                   7c0fdda14eb01       coredns-674b8bbfcf-4h6gp
52904ab472519       499038711c081       7 minutes ago       Running             etcd                      4                   346ff88e66beb       etcd-minikube
0f7d7478589b3       ef43894fa110c       8 minutes ago       Exited              kube-controller-manager   5                   3df1995ebac06       kube-controller-manager-minikube
f76334e3aa31d       c6ab243b29f82       8 minutes ago       Running             kube-apiserver            4                   4039491fd4eee       kube-apiserver-minikube
86c0f1ac8d947       398c985c0d950       8 minutes ago       Running             kube-scheduler            4                   fdf342dfd7c2a       kube-scheduler-minikube
7da0fd8d65af4       b79c189b052cd       2 hours ago         Exited              kube-proxy                3                   e247c8902e109       kube-proxy-vcddx
d4bba855d11e5       1cf5f116067c6       2 hours ago         Exited              coredns                   3                   9fad73dfa8c1c       coredns-674b8bbfcf-ldbpq
cd0dcd9e100b2       1cf5f116067c6       2 hours ago         Exited              coredns                   3                   4d903d1f6116c       coredns-674b8bbfcf-4h6gp
1eb0f118e2ad2       398c985c0d950       2 hours ago         Exited              kube-scheduler            3                   bffe216826ee2       kube-scheduler-minikube
52756299b6f18       499038711c081       2 hours ago         Exited              etcd                      3                   df79f7515b962       etcd-minikube
63d5c5eec471e       c6ab243b29f82       2 hours ago         Exited              kube-apiserver            3                   7951beb3c3ea3       kube-apiserver-minikube


==> coredns [33e494c341a5] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [a17957bb290a] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [cd0dcd9e100b] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [d4bba855d11e] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[ERROR] plugin/errors: 2 4943752721735060833.9214103817910113337. HINFO: read udp 10.244.0.9:43359->192.168.65.2:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_15T10_14_27_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 15 Jul 2025 07:14:11 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 15 Jul 2025 12:16:28 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 15 Jul 2025 12:13:44 +0000   Tue, 15 Jul 2025 07:14:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 15 Jul 2025 12:13:44 +0000   Tue, 15 Jul 2025 07:14:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 15 Jul 2025 12:13:44 +0000   Tue, 15 Jul 2025 07:14:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 15 Jul 2025 12:13:44 +0000   Tue, 15 Jul 2025 07:14:12 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029192Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4029192Ki
  pods:               110
System Info:
  Machine ID:                 fd152dafd88341f5b51df4daf031c484
  System UUID:                c9962f07-d80c-4d8f-9980-140f4d81952a
  Boot ID:                    6548bdaa-4e93-4b80-8301-964417054497
  Kernel Version:             5.10.104-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     messaging-app-5cb54d568b-fhctt              0 (0%)        0 (0%)      0 (0%)           0 (0%)         120m
  default                     messaging-app-5cb54d568b-glvbn              0 (0%)        0 (0%)      0 (0%)           0 (0%)         115m
  default                     messaging-app-5cb54d568b-jfkgg              0 (0%)        0 (0%)      0 (0%)           0 (0%)         120m
  ingress-nginx               ingress-nginx-admission-create-72msr        0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m
  ingress-nginx               ingress-nginx-admission-patch-dbdjw         0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m
  ingress-nginx               ingress-nginx-controller-67c5cb88f-g7hw2    100m (5%)     0 (0%)      90Mi (2%)        0 (0%)         7m
  kube-system                 coredns-674b8bbfcf-4h6gp                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     5h1m
  kube-system                 coredns-674b8bbfcf-ldbpq                    100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     5h1m
  kube-system                 etcd-minikube                               100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         5h2m
  kube-system                 kube-apiserver-minikube                     250m (12%)    0 (0%)      0 (0%)           0 (0%)         5h2m
  kube-system                 kube-controller-manager-minikube            200m (10%)    0 (0%)      0 (0%)           0 (0%)         5h2m
  kube-system                 kube-proxy-vcddx                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h1m
  kube-system                 kube-scheduler-minikube                     100m (5%)     0 (0%)      0 (0%)           0 (0%)         5h2m
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h1m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (47%)  0 (0%)
  memory             330Mi (8%)  340Mi (8%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                  From             Message
  ----     ------                             ----                 ----             -------
  Normal   Starting                           7m37s                kube-proxy       
  Normal   Starting                           121m                 kube-proxy       
  Normal   Starting                           132m                 kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  133m                 kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           133m                 kubelet          Starting kubelet.
  Normal   NodeHasSufficientPID               133m (x7 over 133m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            133m                 kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            133m (x8 over 133m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              133m (x8 over 133m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Warning  Rebooted                           132m                 kubelet          Node minikube has been rebooted, boot id: e4602666-a39f-401b-9f60-5fb5d4ee8289
  Normal   RegisteredNode                     132m                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                           121m                 kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  121m                 kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasSufficientMemory            121m (x8 over 121m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              121m (x8 over 121m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               121m (x7 over 121m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            121m                 kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     121m                 node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  8m7s                 kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           8m7s                 kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory            8m6s (x8 over 8m7s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              8m6s (x8 over 8m7s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               8m6s (x7 over 8m7s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            8m6s                 kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                           7m50s                kubelet          Node minikube has been rebooted, boot id: 6548bdaa-4e93-4b80-8301-964417054497
  Normal   RegisteredNode                     7m26s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jul15 11:02] ERROR: earlyprintk= earlyser already used
[  +0.000000] ERROR: earlyprintk= earlyser already used
[  +0.000000] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0x7E, should be 0xDB (20200925/tbprint-173)
[  +0.000000] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.037764] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.040637] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.002829] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +6.245497] hrtimer: interrupt took 6336807 ns
[  +0.860471] grpcfuse: loading out-of-tree module taints kernel.
[Jul15 11:03] clocksource: timekeeping watchdog on CPU1: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.004429] clocksource:                       'hpet' wd_now: 132c8c9b wd_last: 12510f40 mask: ffffffff
[  +0.003296] clocksource:                       'tsc' cs_now: ea8f10be16e8 cs_last: ea8e20f40cc2 mask: ffffffffffffffff
[  +0.006140] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[  +0.005644] clocksource: Checking clocksource tsc synchronization from CPU 1.
[Jul15 12:08] tmpfs: Unknown parameter 'noswap'


==> etcd [52756299b6f1] <==
{"level":"info","ts":"2025-07-15T10:15:06.933153Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-07-15T10:15:06.982625Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-15T10:15:06.983516Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-15T10:15:06.983573Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-15T10:15:06.986747Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-15T10:15:06.988982Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-07-15T10:15:06.989650Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-15T10:15:06.990522Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-15T10:15:06.994687Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-15T10:15:07.000349Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-07-15T10:15:13.398401Z","caller":"traceutil/trace.go:171","msg":"trace[1048577662] transaction","detail":"{read_only:false; response_revision:7049; number_of_response:1; }","duration":"100.0774ms","start":"2025-07-15T10:15:13.294207Z","end":"2025-07-15T10:15:13.394244Z","steps":["trace[1048577662] 'process raft request'  (duration: 94.6838ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:18:29.603347Z","caller":"traceutil/trace.go:171","msg":"trace[1719986615] linearizableReadLoop","detail":"{readStateIndex:9059; appliedIndex:9059; }","duration":"267.5322ms","start":"2025-07-15T10:18:29.335747Z","end":"2025-07-15T10:18:29.603285Z","steps":["trace[1719986615] 'read index received'  (duration: 267.4485ms)","trace[1719986615] 'applied index is now lower than readState.Index'  (duration: 41.9µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T10:18:29.608412Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"269.4703ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T10:18:29.610361Z","caller":"traceutil/trace.go:171","msg":"trace[318433575] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:7336; }","duration":"275.3158ms","start":"2025-07-15T10:18:29.334931Z","end":"2025-07-15T10:18:29.610244Z","steps":["trace[318433575] 'agreement among raft nodes before linearized reading'  (duration: 269.3376ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:18:56.884441Z","caller":"traceutil/trace.go:171","msg":"trace[2080807006] transaction","detail":"{read_only:false; response_revision:7359; number_of_response:1; }","duration":"187.1967ms","start":"2025-07-15T10:18:56.697198Z","end":"2025-07-15T10:18:56.884395Z","steps":["trace[2080807006] 'process raft request'  (duration: 186.889ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:19:07.010184Z","caller":"traceutil/trace.go:171","msg":"trace[441586831] linearizableReadLoop","detail":"{readStateIndex:9092; appliedIndex:9092; }","duration":"614.1073ms","start":"2025-07-15T10:19:06.395449Z","end":"2025-07-15T10:19:07.009559Z","steps":["trace[441586831] 'read index received'  (duration: 614.0032ms)","trace[441586831] 'applied index is now lower than readState.Index'  (duration: 60.9µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T10:19:07.060548Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"664.971ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T10:19:07.060674Z","caller":"traceutil/trace.go:171","msg":"trace[1716530542] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:7362; }","duration":"665.2584ms","start":"2025-07-15T10:19:06.395373Z","end":"2025-07-15T10:19:07.060630Z","steps":["trace[1716530542] 'agreement among raft nodes before linearized reading'  (duration: 664.8161ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-15T10:23:10.094340Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"252.1256ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T10:23:10.094723Z","caller":"traceutil/trace.go:171","msg":"trace[1818227881] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7598; }","duration":"252.4369ms","start":"2025-07-15T10:23:09.842029Z","end":"2025-07-15T10:23:10.094466Z","steps":["trace[1818227881] 'range keys from in-memory index tree'  (duration: 251.5602ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:25:06.385196Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7409}
{"level":"info","ts":"2025-07-15T10:25:06.410792Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7409,"took":"24.412ms","hash":3726327361,"current-db-size-bytes":3035136,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1458176,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-07-15T10:25:06.411042Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3726327361,"revision":7409,"compact-revision":6528}
{"level":"info","ts":"2025-07-15T10:30:06.306124Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7690}
{"level":"info","ts":"2025-07-15T10:30:06.312326Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7690,"took":"4.3448ms","hash":3010866020,"current-db-size-bytes":3035136,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-15T10:30:06.312480Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3010866020,"revision":7690,"compact-revision":7409}
{"level":"info","ts":"2025-07-15T10:33:23.690761Z","caller":"etcdserver/server.go:1476","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-07-15T10:33:23.703650Z","caller":"etcdserver/server.go:2539","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2025-07-15T10:33:23.703903Z","caller":"etcdserver/server.go:2569","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2025-07-15T10:35:05.984335Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7930}
{"level":"info","ts":"2025-07-15T10:35:05.990275Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":7930,"took":"4.1551ms","hash":287537823,"current-db-size-bytes":3035136,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1536000,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-07-15T10:35:05.990733Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":287537823,"revision":7930,"compact-revision":7690}
{"level":"info","ts":"2025-07-15T10:39:00.438359Z","caller":"traceutil/trace.go:171","msg":"trace[1182564062] transaction","detail":"{read_only:false; response_revision:8362; number_of_response:1; }","duration":"131.1998ms","start":"2025-07-15T10:39:00.307104Z","end":"2025-07-15T10:39:00.438304Z","steps":["trace[1182564062] 'process raft request'  (duration: 124.8272ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:39:14.815982Z","caller":"traceutil/trace.go:171","msg":"trace[1542643448] linearizableReadLoop","detail":"{readStateIndex:10355; appliedIndex:10355; }","duration":"122.6566ms","start":"2025-07-15T10:39:14.693286Z","end":"2025-07-15T10:39:14.815943Z","steps":["trace[1542643448] 'read index received'  (duration: 122.6096ms)","trace[1542643448] 'applied index is now lower than readState.Index'  (duration: 18.7µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T10:39:14.817921Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.633ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T10:39:14.818253Z","caller":"traceutil/trace.go:171","msg":"trace[1191428794] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:8373; }","duration":"124.9934ms","start":"2025-07-15T10:39:14.693219Z","end":"2025-07-15T10:39:14.818213Z","steps":["trace[1191428794] 'agreement among raft nodes before linearized reading'  (duration: 122.9111ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:40:05.658386Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8172}
{"level":"info","ts":"2025-07-15T10:40:05.663579Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8172,"took":"3.8109ms","hash":953457518,"current-db-size-bytes":3035136,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1581056,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-07-15T10:40:05.663994Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":953457518,"revision":8172,"compact-revision":7930}
{"level":"info","ts":"2025-07-15T10:40:56.764063Z","caller":"traceutil/trace.go:171","msg":"trace[452059028] transaction","detail":"{read_only:false; response_revision:8457; number_of_response:1; }","duration":"146.4773ms","start":"2025-07-15T10:40:56.617417Z","end":"2025-07-15T10:40:56.763896Z","steps":["trace[452059028] 'process raft request'  (duration: 144.2581ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:43:39.413399Z","caller":"traceutil/trace.go:171","msg":"trace[1020729183] transaction","detail":"{read_only:false; response_revision:8586; number_of_response:1; }","duration":"241.5298ms","start":"2025-07-15T10:43:39.171832Z","end":"2025-07-15T10:43:39.413364Z","steps":["trace[1020729183] 'process raft request'  (duration: 240.8705ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:43:39.633731Z","caller":"traceutil/trace.go:171","msg":"trace[747626750] transaction","detail":"{read_only:false; response_revision:8587; number_of_response:1; }","duration":"431.2382ms","start":"2025-07-15T10:43:39.201333Z","end":"2025-07-15T10:43:39.632572Z","steps":["trace[747626750] 'process raft request'  (duration: 402.2959ms)","trace[747626750] 'compare'  (duration: 23.4426ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T10:43:39.686017Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T10:43:39.201154Z","time spent":"432.5851ms","remote":"127.0.0.1:41668","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:8583 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-07-15T10:43:39.710406Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T10:43:39.171793Z","time spent":"390.9732ms","remote":"127.0.0.1:41694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:8579 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-07-15T10:47:23.485329Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"697.5382ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T10:47:23.485576Z","caller":"traceutil/trace.go:171","msg":"trace[59952955] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:8597; }","duration":"697.8558ms","start":"2025-07-15T10:47:22.787683Z","end":"2025-07-15T10:47:23.485539Z","steps":["trace[59952955] 'agreement among raft nodes before linearized reading'  (duration: 697.4325ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:48:31.577150Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8414}
{"level":"info","ts":"2025-07-15T10:48:31.581421Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8414,"took":"3.3486ms","hash":2305846744,"current-db-size-bytes":3035136,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1568768,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-07-15T10:48:31.582038Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2305846744,"revision":8414,"compact-revision":8172}
{"level":"warn","ts":"2025-07-15T10:49:15.226386Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.053ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T10:49:15.226584Z","caller":"traceutil/trace.go:171","msg":"trace[1119797505] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8688; }","duration":"510.9031ms","start":"2025-07-15T10:49:14.715587Z","end":"2025-07-15T10:49:15.226494Z","steps":[],"step_count":0}
{"level":"warn","ts":"2025-07-15T10:49:15.226704Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T10:49:14.715533Z","time spent":"511.1294ms","remote":"127.0.0.1:41618","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-07-15T10:50:26.764026Z","caller":"traceutil/trace.go:171","msg":"trace[514503012] linearizableReadLoop","detail":"{readStateIndex:10826; appliedIndex:10826; }","duration":"306.2792ms","start":"2025-07-15T10:50:26.453471Z","end":"2025-07-15T10:50:26.759724Z","steps":["trace[514503012] 'read index received'  (duration: 306.0072ms)","trace[514503012] 'applied index is now lower than readState.Index'  (duration: 210.2µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T10:50:26.773388Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"319.6854ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T10:50:26.773698Z","caller":"traceutil/trace.go:171","msg":"trace[384901158] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:8747; }","duration":"322.1036ms","start":"2025-07-15T10:50:26.446405Z","end":"2025-07-15T10:50:26.773635Z","steps":["trace[384901158] 'agreement among raft nodes before linearized reading'  (duration: 321.3518ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T10:53:31.246647Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8653}
{"level":"info","ts":"2025-07-15T10:53:31.252055Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":8653,"took":"2.8034ms","hash":494966320,"current-db-size-bytes":3035136,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1556480,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-07-15T10:53:31.252739Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":494966320,"revision":8653,"compact-revision":8414}
{"level":"info","ts":"2025-07-15T10:55:56.390025Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-07-15T10:55:56.391389Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [52904ab47251] <==
{"level":"info","ts":"2025-07-15T12:08:32.060542Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-07-15T12:08:32.050482Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-07-15T12:08:32.050660Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-07-15T12:08:32.050805Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-15T12:08:32.079861Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-15T12:08:32.668557Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2025-07-15T12:08:32.668746Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2025-07-15T12:08:32.669377Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-07-15T12:08:32.669511Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2025-07-15T12:08:32.670605Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2025-07-15T12:08:32.670722Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2025-07-15T12:08:32.670897Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2025-07-15T12:08:32.682772Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-07-15T12:08:32.684238Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-15T12:08:32.689876Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-07-15T12:08:32.695557Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-15T12:08:32.698756Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-07-15T12:08:32.698997Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-07-15T12:08:32.702993Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-07-15T12:08:32.710211Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-07-15T12:08:32.730194Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-07-15T12:08:44.427431Z","caller":"traceutil/trace.go:171","msg":"trace[1860788213] transaction","detail":"{read_only:false; response_revision:9092; number_of_response:1; }","duration":"105.5029ms","start":"2025-07-15T12:08:44.321888Z","end":"2025-07-15T12:08:44.427390Z","steps":["trace[1860788213] 'process raft request'  (duration: 105.2412ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T12:08:44.428380Z","caller":"traceutil/trace.go:171","msg":"trace[306044288] transaction","detail":"{read_only:false; response_revision:9091; number_of_response:1; }","duration":"125.9413ms","start":"2025-07-15T12:08:44.302402Z","end":"2025-07-15T12:08:44.428347Z","steps":["trace[306044288] 'process raft request'  (duration: 124.3447ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T12:08:44.428768Z","caller":"traceutil/trace.go:171","msg":"trace[1435289450] linearizableReadLoop","detail":"{readStateIndex:11247; appliedIndex:11246; }","duration":"107.1283ms","start":"2025-07-15T12:08:44.321611Z","end":"2025-07-15T12:08:44.428739Z","steps":["trace[1435289450] 'read index received'  (duration: 104.976ms)","trace[1435289450] 'applied index is now lower than readState.Index'  (duration: 2.1232ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:44.430368Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.7293ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:generic-garbage-collector\" limit:1 ","response":"range_response_count:1 size:775"}
{"level":"info","ts":"2025-07-15T12:08:44.430557Z","caller":"traceutil/trace.go:171","msg":"trace[292217903] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:generic-garbage-collector; range_end:; response_count:1; response_revision:9092; }","duration":"108.9451ms","start":"2025-07-15T12:08:44.321532Z","end":"2025-07-15T12:08:44.430477Z","steps":["trace[292217903] 'agreement among raft nodes before linearized reading'  (duration: 107.3086ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-15T12:08:44.742346Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"159.1733ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:namespace-controller\" limit:1 ","response":"range_response_count:1 size:755"}
{"level":"info","ts":"2025-07-15T12:08:44.742526Z","caller":"traceutil/trace.go:171","msg":"trace[145572635] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:namespace-controller; range_end:; response_count:1; response_revision:9093; }","duration":"159.3649ms","start":"2025-07-15T12:08:44.583071Z","end":"2025-07-15T12:08:44.742437Z","steps":["trace[145572635] 'agreement among raft nodes before linearized reading'  (duration: 159.0313ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T12:08:44.915393Z","caller":"traceutil/trace.go:171","msg":"trace[1649811207] linearizableReadLoop","detail":"{readStateIndex:11249; appliedIndex:11249; }","duration":"332.1928ms","start":"2025-07-15T12:08:44.583152Z","end":"2025-07-15T12:08:44.915345Z","steps":["trace[1649811207] 'read index received'  (duration: 21.5544ms)","trace[1649811207] 'applied index is now lower than readState.Index'  (duration: 82.7483ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:45.055216Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.4196ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038613336030343 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/coredns-674b8bbfcf-4h6gp.18526b6d7d0480ec\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/coredns-674b8bbfcf-4h6gp.18526b6d7d0480ec\" value_size:635 lease:8128038613336030080 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-07-15T12:08:45.092505Z","caller":"traceutil/trace.go:171","msg":"trace[2147008496] transaction","detail":"{read_only:false; response_revision:9094; number_of_response:1; }","duration":"153.5824ms","start":"2025-07-15T12:08:44.938816Z","end":"2025-07-15T12:08:45.092398Z","steps":["trace[2147008496] 'compare'  (duration: 55.5454ms)","trace[2147008496] 'store kv pair into bolt db' {req_type:put; key:/registry/events/kube-system/coredns-674b8bbfcf-4h6gp.18526b6d7d0480ec; req_size:720; } (duration: 31.7019ms)","trace[2147008496] 'attach lease to kv pair' {req_type:put; key:/registry/events/kube-system/coredns-674b8bbfcf-4h6gp.18526b6d7d0480ec; req_size:720; } (duration: 17.9979ms)"],"step_count":3}
{"level":"info","ts":"2025-07-15T12:08:45.099272Z","caller":"traceutil/trace.go:171","msg":"trace[998396362] linearizableReadLoop","detail":"{readStateIndex:11250; appliedIndex:11249; }","duration":"101.4105ms","start":"2025-07-15T12:08:44.997809Z","end":"2025-07-15T12:08:45.099216Z","steps":["trace[998396362] 'read index received'  (duration: 23.9997ms)","trace[998396362] 'applied index is now lower than readState.Index'  (duration: 77.3597ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:45.477581Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"479.4322ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T12:08:45.479419Z","caller":"traceutil/trace.go:171","msg":"trace[51666019] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:9094; }","duration":"481.1862ms","start":"2025-07-15T12:08:44.997755Z","end":"2025-07-15T12:08:45.478935Z","steps":["trace[51666019] 'agreement among raft nodes before linearized reading'  (duration: 479.169ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T12:08:45.482936Z","caller":"traceutil/trace.go:171","msg":"trace[507308511] linearizableReadLoop","detail":"{readStateIndex:11250; appliedIndex:11250; }","duration":"383.4239ms","start":"2025-07-15T12:08:45.099414Z","end":"2025-07-15T12:08:45.482835Z","steps":["trace[507308511] 'read index received'  (duration: 383.3236ms)","trace[507308511] 'applied index is now lower than readState.Index'  (duration: 58.4µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:45.484215Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"462.8581ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T12:08:45.484644Z","caller":"traceutil/trace.go:171","msg":"trace[1561909671] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9094; }","duration":"463.4297ms","start":"2025-07-15T12:08:45.021059Z","end":"2025-07-15T12:08:45.484573Z","steps":["trace[1561909671] 'agreement among raft nodes before linearized reading'  (duration: 462.4736ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-15T12:08:45.484888Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T12:08:45.021027Z","time spent":"463.8116ms","remote":"127.0.0.1:52712","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-07-15T12:08:45.485780Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"433.2347ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:node-controller\" limit:1 ","response":"range_response_count:1 size:735"}
{"level":"info","ts":"2025-07-15T12:08:45.486054Z","caller":"traceutil/trace.go:171","msg":"trace[1922507757] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:node-controller; range_end:; response_count:1; response_revision:9094; }","duration":"433.4998ms","start":"2025-07-15T12:08:45.052425Z","end":"2025-07-15T12:08:45.485925Z","steps":["trace[1922507757] 'agreement among raft nodes before linearized reading'  (duration: 433.1426ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-15T12:08:45.486205Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T12:08:45.052360Z","time spent":"433.8098ms","remote":"127.0.0.1:52800","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":759,"request content":"key:\"/registry/clusterrolebindings/system:controller:node-controller\" limit:1 "}
{"level":"warn","ts":"2025-07-15T12:08:46.752244Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"246.7442ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:replication-controller\" limit:1 ","response":"range_response_count:1 size:763"}
{"level":"info","ts":"2025-07-15T12:08:46.754621Z","caller":"traceutil/trace.go:171","msg":"trace[816369945] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:replication-controller; range_end:; response_count:1; response_revision:9098; }","duration":"249.1502ms","start":"2025-07-15T12:08:46.505419Z","end":"2025-07-15T12:08:46.754568Z","steps":["trace[816369945] 'agreement among raft nodes before linearized reading'  (duration: 174.0904ms)","trace[816369945] 'range keys from in-memory index tree'  (duration: 72.4072ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:46.754043Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"552.5932ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T12:08:46.761792Z","caller":"traceutil/trace.go:171","msg":"trace[711021363] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9098; }","duration":"663.6326ms","start":"2025-07-15T12:08:46.011074Z","end":"2025-07-15T12:08:46.761692Z","steps":["trace[711021363] 'agreement among raft nodes before linearized reading'  (duration: 581.4833ms)","trace[711021363] 'range keys from in-memory index tree'  (duration: 74.3059ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:46.763698Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T12:08:46.011035Z","time spent":"750.8355ms","remote":"127.0.0.1:52712","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-07-15T12:08:46.764536Z","caller":"traceutil/trace.go:171","msg":"trace[1788254460] linearizableReadLoop","detail":"{readStateIndex:11254; appliedIndex:11254; }","duration":"258.0285ms","start":"2025-07-15T12:08:46.505725Z","end":"2025-07-15T12:08:46.764487Z","steps":["trace[1788254460] 'read index received'  (duration: 257.9751ms)","trace[1788254460] 'applied index is now lower than readState.Index'  (duration: 23.9µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:46.886737Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"134.8978ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T12:08:46.886869Z","caller":"traceutil/trace.go:171","msg":"trace[1829335303] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:9098; }","duration":"135.0563ms","start":"2025-07-15T12:08:46.751768Z","end":"2025-07-15T12:08:46.886830Z","steps":["trace[1829335303] 'agreement among raft nodes before linearized reading'  (duration: 128.0259ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T12:08:47.206837Z","caller":"traceutil/trace.go:171","msg":"trace[691186927] linearizableReadLoop","detail":"{readStateIndex:11255; appliedIndex:11255; }","duration":"109.3922ms","start":"2025-07-15T12:08:47.097401Z","end":"2025-07-15T12:08:47.206793Z","steps":["trace[691186927] 'read index received'  (duration: 109.3377ms)","trace[691186927] 'applied index is now lower than readState.Index'  (duration: 25.8µs)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:47.207312Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.8916ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:service-cidrs-controller\" limit:1 ","response":"range_response_count:1 size:771"}
{"level":"info","ts":"2025-07-15T12:08:47.207477Z","caller":"traceutil/trace.go:171","msg":"trace[1626813952] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:service-cidrs-controller; range_end:; response_count:1; response_revision:9099; }","duration":"110.1104ms","start":"2025-07-15T12:08:47.097337Z","end":"2025-07-15T12:08:47.207447Z","steps":["trace[1626813952] 'agreement among raft nodes before linearized reading'  (duration: 109.6232ms)"],"step_count":1}
{"level":"info","ts":"2025-07-15T12:08:49.568844Z","caller":"traceutil/trace.go:171","msg":"trace[801531164] linearizableReadLoop","detail":"{readStateIndex:11273; appliedIndex:11270; }","duration":"336.2575ms","start":"2025-07-15T12:08:49.232547Z","end":"2025-07-15T12:08:49.568805Z","steps":["trace[801531164] 'read index received'  (duration: 292.6208ms)","trace[801531164] 'applied index is now lower than readState.Index'  (duration: 43.6064ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-15T12:08:49.570210Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"337.6865ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-15T12:08:49.571108Z","caller":"traceutil/trace.go:171","msg":"trace[942902344] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:9115; }","duration":"338.6364ms","start":"2025-07-15T12:08:49.232424Z","end":"2025-07-15T12:08:49.571060Z","steps":["trace[942902344] 'agreement among raft nodes before linearized reading'  (duration: 337.4321ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-15T12:08:49.572030Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T12:08:49.232377Z","time spent":"339.3946ms","remote":"127.0.0.1:52712","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-07-15T12:08:49.572182Z","caller":"traceutil/trace.go:171","msg":"trace[1534287087] transaction","detail":"{read_only:false; response_revision:9114; number_of_response:1; }","duration":"439.9555ms","start":"2025-07-15T12:08:49.132183Z","end":"2025-07-15T12:08:49.572138Z","steps":["trace[1534287087] 'process raft request'  (duration: 435.6976ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-15T12:08:49.575115Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T12:08:49.132136Z","time spent":"442.5844ms","remote":"127.0.0.1:52754","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4232,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/kube-system/storage-provisioner\" mod_revision:9076 > success:<request_put:<key:\"/registry/pods/kube-system/storage-provisioner\" value_size:4178 >> failure:<request_range:<key:\"/registry/pods/kube-system/storage-provisioner\" > >"}
{"level":"info","ts":"2025-07-15T12:08:49.572306Z","caller":"traceutil/trace.go:171","msg":"trace[1865524619] transaction","detail":"{read_only:false; response_revision:9115; number_of_response:1; }","duration":"359.6972ms","start":"2025-07-15T12:08:49.212584Z","end":"2025-07-15T12:08:49.572282Z","steps":["trace[1865524619] 'process raft request'  (duration: 355.8284ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-15T12:08:49.580417Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-07-15T12:08:49.212506Z","time spent":"367.6868ms","remote":"127.0.0.1:52730","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":902,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/messaging-app-5cb54d568b-glvbn.18526b6eeeaae94c\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/messaging-app-5cb54d568b-glvbn.18526b6eeeaae94c\" value_size:812 lease:8128038613336030080 >> failure:<>"}


==> kernel <==
 12:16:28 up  1:13,  0 users,  load average: 0.91, 0.86, 0.55
Linux minikube 5.10.104-linuxkit #1 SMP Wed Mar 9 19:05:23 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [63d5c5eec471] <==
I0715 10:15:10.180438       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0715 10:15:10.181401       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0715 10:15:10.181768       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0715 10:15:10.182644       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0715 10:15:10.185422       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0715 10:15:10.186327       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0715 10:15:10.188323       1 repairip.go:200] Starting ipallocator-repair-controller
I0715 10:15:10.188706       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0715 10:15:10.266018       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0715 10:15:10.266210       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0715 10:15:10.267682       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0715 10:15:10.267816       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0715 10:15:10.273692       1 controller.go:142] Starting OpenAPI controller
I0715 10:15:10.274360       1 controller.go:90] Starting OpenAPI V3 controller
I0715 10:15:10.274968       1 naming_controller.go:299] Starting NamingConditionController
I0715 10:15:10.275474       1 establishing_controller.go:81] Starting EstablishingController
I0715 10:15:10.275977       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0715 10:15:10.276526       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0715 10:15:10.277023       1 crd_finalizer.go:269] Starting CRDFinalizer
I0715 10:15:10.275364       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0715 10:15:10.275410       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0715 10:15:10.967445       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0715 10:15:10.974081       1 aggregator.go:171] initial CRD sync complete...
I0715 10:15:10.978514       1 autoregister_controller.go:144] Starting autoregister controller
I0715 10:15:10.978574       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0715 10:15:10.997857       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0715 10:15:11.056507       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 10:15:11.068397       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0715 10:15:11.068535       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0715 10:15:11.088893       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0715 10:15:11.152482       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0715 10:15:11.153775       1 policy_source.go:240] refreshing policies
I0715 10:15:11.166983       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0715 10:15:11.167046       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0715 10:15:11.215745       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0715 10:15:11.273040       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0715 10:15:11.276071       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0715 10:15:11.281350       1 cache.go:39] Caches are synced for LocalAvailability controller
I0715 10:15:11.282848       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0715 10:15:11.289069       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0715 10:15:11.292227       1 cache.go:39] Caches are synced for autoregister controller
I0715 10:15:11.323528       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0715 10:15:11.655881       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0715 10:15:11.659479       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0715 10:15:16.216473       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0715 10:15:16.219585       1 controller.go:667] quota admission added evaluator for: endpoints
I0715 10:15:16.236690       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0715 10:15:18.406424       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 10:15:47.953135       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0715 10:15:47.988633       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0715 10:16:09.974086       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 10:16:09.991308       1 alloc.go:328] "allocated clusterIPs" service="default/messaging-service" clusterIPs={"IPv4":"10.98.170.10"}
I0715 10:25:10.289578       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 10:35:09.860282       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 10:48:35.430957       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 10:55:57.346410       1 controller.go:128] Shutting down kubernetes service endpoint reconciler
W0715 10:55:57.525630       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0715 10:55:58.057270       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0715 10:55:58.057540       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0715 10:55:58.057711       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [f76334e3aa31] <==
I0715 12:08:37.336869       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0715 12:08:37.338447       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0715 12:08:37.340417       1 local_available_controller.go:156] Starting LocalAvailability controller
I0715 12:08:37.340755       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0715 12:08:37.341779       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0715 12:08:37.342149       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0715 12:08:37.343007       1 controller.go:78] Starting OpenAPI AggregationController
I0715 12:08:37.344538       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0715 12:08:37.345748       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0715 12:08:37.352486       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0715 12:08:37.354797       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0715 12:08:37.359448       1 repairip.go:200] Starting ipallocator-repair-controller
I0715 12:08:37.360254       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0715 12:08:37.420079       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0715 12:08:37.420159       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0715 12:08:37.445930       1 controller.go:142] Starting OpenAPI controller
I0715 12:08:37.447256       1 controller.go:90] Starting OpenAPI V3 controller
I0715 12:08:37.447822       1 naming_controller.go:299] Starting NamingConditionController
I0715 12:08:37.448395       1 establishing_controller.go:81] Starting EstablishingController
I0715 12:08:37.449084       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0715 12:08:37.449603       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0715 12:08:37.450318       1 crd_finalizer.go:269] Starting CRDFinalizer
I0715 12:08:38.256171       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0715 12:08:38.277721       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0715 12:08:38.277858       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0715 12:08:38.278891       1 aggregator.go:171] initial CRD sync complete...
I0715 12:08:38.286809       1 autoregister_controller.go:144] Starting autoregister controller
I0715 12:08:38.286840       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0715 12:08:38.291414       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 12:08:38.318830       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0715 12:08:38.321273       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0715 12:08:38.346122       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0715 12:08:38.346591       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0715 12:08:38.346716       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0715 12:08:38.360284       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0715 12:08:38.360366       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0715 12:08:38.361864       1 cache.go:39] Caches are synced for LocalAvailability controller
I0715 12:08:38.372362       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0715 12:08:38.372443       1 policy_source.go:240] refreshing policies
I0715 12:08:38.386977       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0715 12:08:38.387236       1 cache.go:39] Caches are synced for autoregister controller
I0715 12:08:38.395497       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0715 12:08:38.470017       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0715 12:08:38.555354       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
E0715 12:08:38.644023       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0715 12:08:39.543012       1 controller.go:667] quota admission added evaluator for: serviceaccounts
E0715 12:08:48.808816       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: f63cf4e7-330c-4b6a-9571-e26e32841183, UID in object meta: "
I0715 12:09:02.648018       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 12:09:02.729063       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0715 12:09:02.790960       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0715 12:09:02.907113       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0715 12:09:02.958856       1 controller.go:667] quota admission added evaluator for: endpoints
I0715 12:09:27.383311       1 controller.go:667] quota admission added evaluator for: namespaces
I0715 12:09:27.578193       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0715 12:09:27.708495       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0715 12:09:27.957860       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 12:09:27.981530       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.104.154.227"}
I0715 12:09:28.109968       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0715 12:09:28.113821       1 alloc.go:328] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.102.16.140"}
I0715 12:09:28.246729       1 controller.go:667] quota admission added evaluator for: jobs.batch


==> kube-controller-manager [0f7d7478589b] <==
I0715 12:08:33.850057       1 serving.go:386] Generated self-signed cert in-memory
I0715 12:08:35.441679       1 controllermanager.go:188] "Starting" version="v1.33.1"
I0715 12:08:35.442392       1 controllermanager.go:190] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0715 12:08:35.459233       1 secure_serving.go:211] Serving securely on 127.0.0.1:10257
I0715 12:08:35.467720       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0715 12:08:35.473731       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0715 12:08:35.469497       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
E0715 12:08:47.721759       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-kubernetes-service-cidr-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-controller-manager [2407846648db] <==
I0715 12:09:02.355793       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0715 12:09:02.358402       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-legacy-unknown"
I0715 12:09:02.358474       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrsigning-legacy-unknown"
I0715 12:09:02.358604       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0715 12:09:02.353952       1 certificate_controller.go:120] "Starting certificate controller" logger="certificatesigningrequest-signing-controller" name="csrsigning-kubelet-serving"
I0715 12:09:02.359968       1 shared_informer.go:350] "Waiting for caches to sync" controller="certificate-csrsigning-kubelet-serving"
I0715 12:09:02.354004       1 dynamic_serving_content.go:135] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0715 12:09:02.405081       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0715 12:09:02.501471       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0715 12:09:02.513533       1 shared_informer.go:357] "Caches are synced" controller="node"
I0715 12:09:02.516564       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0715 12:09:02.517208       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0715 12:09:02.517599       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0715 12:09:02.518276       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0715 12:09:02.521450       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0715 12:09:02.522071       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0715 12:09:02.527663       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0715 12:09:02.530220       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0715 12:09:02.548160       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0715 12:09:02.549604       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0715 12:09:02.555778       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0715 12:09:02.556708       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0715 12:09:02.558604       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0715 12:09:02.559578       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0715 12:09:02.560266       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0715 12:09:02.569058       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0715 12:09:02.572041       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0715 12:09:02.578620       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0715 12:09:02.581996       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0715 12:09:02.586228       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0715 12:09:02.599465       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0715 12:09:02.602433       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0715 12:09:02.603015       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0715 12:09:02.607280       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0715 12:09:02.626562       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0715 12:09:02.630031       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0715 12:09:02.630956       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0715 12:09:02.631827       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0715 12:09:02.634619       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0715 12:09:02.635204       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0715 12:09:02.639316       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0715 12:09:02.646844       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0715 12:09:02.651025       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0715 12:09:02.656911       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0715 12:09:02.663339       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0715 12:09:02.663817       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0715 12:09:02.665572       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0715 12:09:02.719728       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0715 12:09:02.723203       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0715 12:09:02.742799       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0715 12:09:02.759997       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0715 12:09:02.762786       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0715 12:09:02.768507       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0715 12:09:02.774869       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0715 12:09:02.776387       1 shared_informer.go:357] "Caches are synced" controller="job"
I0715 12:09:02.806978       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0715 12:09:03.182987       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0715 12:09:03.251906       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0715 12:09:03.251967       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0715 12:09:03.251984       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [3e8394f6cc35] <==
I0715 12:08:51.159211       1 server_linux.go:63] "Using iptables proxy"
I0715 12:08:51.334703       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0715 12:08:51.335176       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0715 12:08:51.409346       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0715 12:08:51.409433       1 server_linux.go:145] "Using iptables Proxier"
I0715 12:08:51.427949       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0715 12:08:51.429194       1 server.go:516] "Version info" version="v1.33.1"
I0715 12:08:51.429531       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0715 12:08:51.447677       1 config.go:199] "Starting service config controller"
I0715 12:08:51.448288       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0715 12:08:51.449681       1 config.go:105] "Starting endpoint slice config controller"
I0715 12:08:51.449901       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0715 12:08:51.450415       1 config.go:440] "Starting serviceCIDR config controller"
I0715 12:08:51.450771       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0715 12:08:51.463648       1 config.go:329] "Starting node config controller"
I0715 12:08:51.464278       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0715 12:08:51.550825       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0715 12:08:51.551435       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0715 12:08:51.553073       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0715 12:08:51.564587       1 shared_informer.go:357] "Caches are synced" controller="node config"


==> kube-proxy [7da0fd8d65af] <==
I0715 10:15:16.046408       1 server_linux.go:63] "Using iptables proxy"
I0715 10:15:16.219767       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0715 10:15:16.220379       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0715 10:15:16.284454       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0715 10:15:16.284589       1 server_linux.go:145] "Using iptables Proxier"
I0715 10:15:16.299032       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0715 10:15:16.301009       1 server.go:516] "Version info" version="v1.33.1"
I0715 10:15:16.301096       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0715 10:15:16.315473       1 config.go:199] "Starting service config controller"
I0715 10:15:16.315841       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0715 10:15:16.316509       1 config.go:105] "Starting endpoint slice config controller"
I0715 10:15:16.316627       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0715 10:15:16.316762       1 config.go:440] "Starting serviceCIDR config controller"
I0715 10:15:16.317031       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0715 10:15:16.322783       1 config.go:329] "Starting node config controller"
I0715 10:15:16.322840       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0715 10:15:16.423725       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0715 10:15:16.424619       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0715 10:15:16.424871       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0715 10:15:16.427637       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [1eb0f118e2ad] <==
I0715 10:15:08.313010       1 serving.go:386] Generated self-signed cert in-memory
W0715 10:15:10.324385       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0715 10:15:10.324780       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0715 10:15:10.326058       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0715 10:15:10.330355       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0715 10:15:10.680793       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0715 10:15:10.680879       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0715 10:15:10.712828       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0715 10:15:10.713187       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0715 10:15:10.721833       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0715 10:15:10.722316       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0715 10:15:10.887273       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0715 10:15:10.902984       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0715 10:15:10.966542       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0715 10:15:10.977752       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0715 10:15:10.988573       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0715 10:15:10.989082       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0715 10:15:10.989370       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0715 10:15:10.989651       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0715 10:15:10.998741       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0715 10:15:10.998928       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0715 10:15:10.999413       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0715 10:15:10.999685       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0715 10:15:11.045421       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0715 10:15:11.050215       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0715 10:15:11.051016       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0715 10:15:11.051265       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
I0715 10:15:12.583812       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0715 10:55:57.444975       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [86c0f1ac8d94] <==
I0715 12:08:33.472276       1 serving.go:386] Generated self-signed cert in-memory
W0715 12:08:37.521345       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0715 12:08:37.521629       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0715 12:08:37.521837       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0715 12:08:37.522276       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0715 12:08:37.959487       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0715 12:08:37.959571       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0715 12:08:37.969556       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0715 12:08:37.973036       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0715 12:08:37.974955       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0715 12:08:37.975647       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
E0715 12:08:38.088271       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0715 12:08:38.088364       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0715 12:08:38.088468       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0715 12:08:38.088584       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0715 12:08:38.089417       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0715 12:08:38.101325       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0715 12:08:38.106168       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0715 12:08:38.106512       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0715 12:08:38.106727       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0715 12:08:38.107062       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0715 12:08:38.107470       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0715 12:08:38.107765       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0715 12:08:38.108053       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0715 12:08:38.174890       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0715 12:08:38.183567       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0715 12:08:38.187174       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
I0715 12:08:39.200392       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jul 15 12:14:40 minikube kubelet[1506]: E0715 12:14:40.663129    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:14:40 minikube kubelet[1506]: E0715 12:14:40.665576    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:14:45 minikube kubelet[1506]: E0715 12:14:45.665835    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:14:45 minikube kubelet[1506]: E0715 12:14:45.667202    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:14:46 minikube kubelet[1506]: E0715 12:14:46.662824    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:14:46 minikube kubelet[1506]: E0715 12:14:46.665684    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:14:53 minikube kubelet[1506]: E0715 12:14:53.666252    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:14:53 minikube kubelet[1506]: E0715 12:14:53.668510    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:14:59 minikube kubelet[1506]: E0715 12:14:59.631648    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:14:59 minikube kubelet[1506]: E0715 12:14:59.635175    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:15:00 minikube kubelet[1506]: E0715 12:15:00.629545    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:00 minikube kubelet[1506]: E0715 12:15:00.632015    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:15:06 minikube kubelet[1506]: E0715 12:15:06.629944    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:06 minikube kubelet[1506]: E0715 12:15:06.634148    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:15:12 minikube kubelet[1506]: E0715 12:15:12.629081    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:12 minikube kubelet[1506]: E0715 12:15:12.631208    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:15:13 minikube kubelet[1506]: E0715 12:15:13.628470    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:13 minikube kubelet[1506]: E0715 12:15:13.630195    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:15:20 minikube kubelet[1506]: E0715 12:15:20.628327    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:20 minikube kubelet[1506]: E0715 12:15:20.630241    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:15:25 minikube kubelet[1506]: E0715 12:15:25.593879    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:25 minikube kubelet[1506]: E0715 12:15:25.600854    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:15:27 minikube kubelet[1506]: E0715 12:15:27.594670    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:27 minikube kubelet[1506]: E0715 12:15:27.597173    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:15:34 minikube kubelet[1506]: E0715 12:15:34.593772    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:34 minikube kubelet[1506]: E0715 12:15:34.595529    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:15:40 minikube kubelet[1506]: E0715 12:15:40.594427    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:40 minikube kubelet[1506]: E0715 12:15:40.596537    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:15:41 minikube kubelet[1506]: E0715 12:15:41.170774    1506 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jul 15 12:15:41 minikube kubelet[1506]: E0715 12:15:41.171215    1506 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/170612a3-f0e2-46c8-ab2a-0cee36717c84-webhook-cert podName:170612a3-f0e2-46c8-ab2a-0cee36717c84 nodeName:}" failed. No retries permitted until 2025-07-15 12:17:43.1711674 +0000 UTC m=+563.258114701 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/170612a3-f0e2-46c8-ab2a-0cee36717c84-webhook-cert") pod "ingress-nginx-controller-67c5cb88f-g7hw2" (UID: "170612a3-f0e2-46c8-ab2a-0cee36717c84") : secret "ingress-nginx-admission" not found
Jul 15 12:15:42 minikube kubelet[1506]: E0715 12:15:42.594101    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:42 minikube kubelet[1506]: E0715 12:15:42.595955    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:15:46 minikube kubelet[1506]: E0715 12:15:46.594731    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:46 minikube kubelet[1506]: E0715 12:15:46.597566    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:15:46 minikube kubelet[1506]: E0715 12:15:46.929824    1506 log.go:32] "PullImage from image service failed" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Jul 15 12:15:46 minikube kubelet[1506]: E0715 12:15:46.930153    1506 kuberuntime_image.go:42] "Failed to pull image" err="rpc error: code = Canceled desc = context canceled" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524"
Jul 15 12:15:46 minikube kubelet[1506]: E0715 12:15:46.930770    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:create,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524,Command:[],Args:[create --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc --namespace=$(POD_NAMESPACE) --secret-name=ingress-nginx-admission],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4m5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-create-72msr_ingress-nginx(96a8afe1-86c8-4484-97e5-753f1c2b9b6e): ErrImagePull: rpc error: code = Canceled desc = context canceled" logger="UnhandledError"
Jul 15 12:15:46 minikube kubelet[1506]: E0715 12:15:46.933074    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-72msr" podUID="96a8afe1-86c8-4484-97e5-753f1c2b9b6e"
Jul 15 12:15:55 minikube kubelet[1506]: E0715 12:15:55.560541    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:55 minikube kubelet[1506]: E0715 12:15:55.567820    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:15:55 minikube kubelet[1506]: E0715 12:15:55.569035    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:55 minikube kubelet[1506]: E0715 12:15:55.576975    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:15:58 minikube kubelet[1506]: E0715 12:15:58.561529    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-67c5cb88f-g7hw2" podUID="170612a3-f0e2-46c8-ab2a-0cee36717c84"
Jul 15 12:15:59 minikube kubelet[1506]: E0715 12:15:59.572346    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:15:59 minikube kubelet[1506]: E0715 12:15:59.576217    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:16:00 minikube kubelet[1506]: E0715 12:16:00.570634    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.5.3@sha256:2cf4ebfa82a37c357455458f6dfc334aea1392d508270b2517795a9933a02524\\\": ErrImagePull: rpc error: code = Canceled desc = context canceled\"" pod="ingress-nginx/ingress-nginx-admission-create-72msr" podUID="96a8afe1-86c8-4484-97e5-753f1c2b9b6e"
Jul 15 12:16:06 minikube kubelet[1506]: E0715 12:16:06.560835    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:16:06 minikube kubelet[1506]: E0715 12:16:06.562121    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:16:06 minikube kubelet[1506]: E0715 12:16:06.563117    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:16:06 minikube kubelet[1506]: E0715 12:16:06.565790    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:16:11 minikube kubelet[1506]: E0715 12:16:11.560780    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:16:11 minikube kubelet[1506]: E0715 12:16:11.563264    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:16:18 minikube kubelet[1506]: E0715 12:16:18.559956    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:16:18 minikube kubelet[1506]: E0715 12:16:18.562193    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"
Jul 15 12:16:19 minikube kubelet[1506]: E0715 12:16:19.573759    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-jfkgg_default(34a296da-755a-4947-8474-af89d97af135): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:16:19 minikube kubelet[1506]: E0715 12:16:19.582111    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-jfkgg" podUID="34a296da-755a-4947-8474-af89d97af135"
Jul 15 12:16:23 minikube kubelet[1506]: E0715 12:16:23.560848    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-fhctt_default(f8034dac-be97-4e91-8d85-9749abc23e90): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:16:23 minikube kubelet[1506]: E0715 12:16:23.562056    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-fhctt" podUID="f8034dac-be97-4e91-8d85-9749abc23e90"
Jul 15 12:16:29 minikube kubelet[1506]: E0715 12:16:29.528636    1506 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:messaging-app,Image:<your-dockerhub-username>/messaging-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krj56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod messaging-app-5cb54d568b-glvbn_default(d3f9e601-bfa9-427e-8c63-761a37820331): InvalidImageName: Failed to apply default image tag \"<your-dockerhub-username>/messaging-app:latest\": couldn't parse image name \"<your-dockerhub-username>/messaging-app:latest\": invalid reference format" logger="UnhandledError"
Jul 15 12:16:29 minikube kubelet[1506]: E0715 12:16:29.530345    1506 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"messaging-app\" with InvalidImageName: \"Failed to apply default image tag \\\"<your-dockerhub-username>/messaging-app:latest\\\": couldn't parse image name \\\"<your-dockerhub-username>/messaging-app:latest\\\": invalid reference format\"" pod="default/messaging-app-5cb54d568b-glvbn" podUID="d3f9e601-bfa9-427e-8c63-761a37820331"


==> storage-provisioner [7539bb12576d] <==
W0715 12:15:29.106688       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:29.123050       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:31.134903       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:31.149818       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:33.157850       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:33.170446       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:35.182667       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:35.193994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:37.203616       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:37.218370       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:39.225893       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:39.241007       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:41.254898       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:41.274371       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:43.284677       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:43.297581       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:45.305913       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:45.320662       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:47.327648       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:47.340907       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:49.515647       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:49.584117       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:51.614664       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:51.646494       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:53.655193       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:53.668764       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:55.647275       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:55.664171       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:57.716523       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:57.828640       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:59.839660       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:15:59.859134       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:01.870497       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:01.895797       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:03.919224       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:03.953235       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:05.963638       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:05.979062       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:08.002585       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:08.019374       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:10.064755       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:10.104882       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:12.164060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:12.191727       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:14.209118       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:14.243720       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:16.258222       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:16.275570       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:18.284959       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:18.303276       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:20.313843       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:20.329699       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:22.344985       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:22.372898       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:24.380083       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:24.391629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:26.373785       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:26.399775       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:28.410662       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0715 12:16:28.439335       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [c5854b86f6c5] <==
I0715 12:08:50.104031       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0715 12:09:20.262360       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

